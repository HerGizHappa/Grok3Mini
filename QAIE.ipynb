{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9BCNOvT1auc"
      },
      "source": [
        "# Setup & Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nhCh6T2Crh3u"
      },
      "outputs": [],
      "source": [
        "'''# Schritt 1: Deinstalliere alle problematischen Pakete\n",
        "!pip uninstall tensorflow tensorflow-text tf-keras tensorflow-decision-forests tensorflow-probability -y\n",
        "\n",
        "# Schritt 2: Installiere kompatible Versionen\n",
        "!pip install tensorflow==2.19.0 tensorflow-probability==0.25.0 tensorflow-text==2.19.0 tf-keras==2.19.0\n",
        "\n",
        "!pip install -q sentence-transformers transformers accelerate torch pandas tqdm\n",
        "\n",
        "# Schritt 3: Unterdrücke Warnungen (für parakeet und CUDA)\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Unterdrückt TensorFlow-Warnungen\n",
        "\n",
        "# Schritt 4: Teste den Import\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "print(\"Import erfolgreich!\")\n",
        "\n",
        "# Schritt 5: Teste tensorflow_probability\n",
        "import tensorflow_probability as tfp\n",
        "print(f\"TensorFlow Probability Version: {tfp.__version__}\")\n",
        "!pip install transformers torch pandas tqdm -q'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhqmpRX0F-WE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import platform #infos aus dem System verarbeiten können (Windows)\n",
        "import os #für Dateihändelung\n",
        "import pandas as pd #für Datenanalyse (Bereinigung von Daten)\n",
        "import random\n",
        "from tqdm import tqdm #um ausgeben zu können, wie weit die Prozesse laufen (für Prozess Balken)\n",
        "import tensorflow_probability as tfp # Wahrscheinlichekti von Ergebnissen analysieren & bewerten\n",
        "import torch # soll dabei helfen, schnellere Berechnungen zu machen (GPU)\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM #aus hugging face bibliothek, um Texte in Tokens umzuwandeln, Textsequenzen als Input und andere Textsequenzen ausgeben (Output)\n",
        "#Aufgabe: Übersetzen, Zusammenfassen, Generieren\n",
        "from sentence_transformers import SentenceTransformer, util #sentenceTranformer, damit werden Modelle geladen, die Texte in ganze Vektoren umwandeln\n",
        "#ür Sätze/Texte erzeugen → Ähnlichkeitsvergleiche\n",
        "from google.colab import files, drive #Interaktion mit lokalen Dateien in Google Colab Umgebung\n",
        "from datetime import datetime# Manipulation von Zeitangaben\n",
        "import shutil #Operationen mit Dateien und mit dem Sammeln von Dateien\n",
        "import re #Operationen für reguläre Ausdrücke (regex)\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "# NLTK Punkt-Tokenizer\n",
        "nltk.download('punkt', quiet=True)\n",
        "import math\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqgyTXTb2AqA"
      },
      "outputs": [],
      "source": [
        "def smartEncoding():\n",
        "  plt = platform.system\n",
        "  if plt == \"Windows\":\n",
        "    return \"utf-8-sig\"\n",
        "  else:\n",
        "    return \"utf-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6sxMd7fsqOR"
      },
      "source": [
        "# Gewählte xlsl Datei in csv Datei umwandeln und umbenennen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VaFrz040hSDp"
      },
      "outputs": [],
      "source": [
        "# title\n",
        "\n",
        "read_file = pd.DataFrame(pd.read_excel('/content/drive/My Drive/Google_Rezensionen_qualitätsgesichert_2023_12_19.xlsx'))\n",
        "read_file.to_csv(\"Google_Rezensionen_qualitätsgesichert_2023_12_19.csv\", index = False, header = True, encoding=smartEncoding())\n",
        "df = pd.DataFrame(pd.read_csv(\"Google_Rezensionen_qualitätsgesichert_2023_12_19.csv\"))\n",
        "#print(df.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q4FshCWEs1-J"
      },
      "outputs": [],
      "source": [
        "#wähle relevante Spalten\n",
        "gewaehlte_spalten = df[[\"Erfahrungsbericht des Nutzers\" , \"Zufallszahl\"]]\n",
        "\n",
        "#speichere sie in neue csv datei\n",
        "gewaehlte_spalten.to_csv(\"/content/drive/My Drive/erfahrungen_gefiltert.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfd5RwKdjNQv"
      },
      "source": [
        "# Datenbereinigung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "etZLmS_XhRox"
      },
      "outputs": [],
      "source": [
        "erfahrungen_gefiltert = pd.read_csv(\"/content/drive/My Drive/erfahrungen_gefiltert.csv\")\n",
        "# dropna() entfernt NaN-Reihen aus anderen Spalten\n",
        "erfahrungen_gefiltert.dropna(subset=['Erfahrungsbericht des Nutzers'], inplace=True)\n",
        "# leere strings entfernen\n",
        "erfahrungen_gefiltert.drop_duplicates(inplace=True)\n",
        "#speichern\n",
        "erfahrungen_gefiltert.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index=False)\n",
        "print(f\"FINAL: {len(erfahrungen_gefiltert)} Reihen\")\n",
        "\n",
        "#speichere sie in neue csv datei\n",
        "gewaehlte_spalten.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index = False)\n",
        "\n",
        "#zufällig n = 100 Berichte für erfahrungen_final.csv auswählen\n",
        "erfahrungen_clean = pd.read_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\")\n",
        "# dropna() entfernt NaN-Reihen aus anderen Spalten\n",
        "erfahrungen_clean.dropna(subset=['Erfahrungsbericht des Nutzers'], inplace=True)\n",
        "# leere strings entfernen\n",
        "erfahrungen_clean.drop_duplicates(inplace=True)\n",
        "\n",
        "erfahrungen_clean.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index = False)\n",
        "# INDEX-Nummern zufällig wählen\n",
        "zufalls_index = random.sample(range(len(erfahrungen_clean)), k=100)\n",
        "zufall_100 = erfahrungen_clean.iloc[zufalls_index]\n",
        "#speichern (NUR 100!)\n",
        "zufall_100.to_csv(\"/content/drive/My Drive/zufall_100_berichte.csv\", index=False)\n",
        "# selben Rezensionen wurden für händische ABSA in seperaten Datei \"goldstandard_clean\" gespeichert\n",
        "\n",
        "\n",
        "print(f\"{len(zufall_100)} Zufallsberichte!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPzrsAJjFsW2"
      },
      "source": [
        "# **Datenaugmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDX0N5V-IhnY"
      },
      "source": [
        "## Datenaugmentation mit ca. 11.900 Rezensionen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p0Z4ygud_8_"
      },
      "source": [
        "## Paraphrasieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkXOEJrCaMv2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Google Drive mounten/ Ordner in Google Colab Umbegung erstellen lassen (einmalige Ausführung nötig)\n",
        "drive.mount('/content/drive')\n",
        "SAVE_PATH = '/content/drive/MyDrive/Projekt_ABSA' #Unterordner \"Projekt_ABSA\" in Google Colab Umgebung erstellen\n",
        "os.makedirs(SAVE_PATH, exist_ok=True) #Methode, wobei ein Projektverzeichnis (Pfad für Paraphrase mit inbegriffen) festgelegt wird\n",
        "\n",
        "print(\"Ordner & Bibliotheken geladen!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fDHUXUvU3mLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers nltk -q"
      ],
      "metadata": {
        "id": "VoDU2GopQ_hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "def smartEncoding():\n",
        "  plt = platform.system\n",
        "  if plt == \"Windows\":\n",
        "    return \"utf-8-sig\"\n",
        "  else:\n",
        "    return \"utf-8\"\n",
        "\n",
        "# Erstellung eines \"Synonym-Dictionarys\", zuerste mit Schlüsselwert wie z.B. \"schmutzig\" & danach Wertpaaren\n",
        "SYNONYMS = {\n",
        "    \"Personal\": [\"Team\", \"Mitarbeiter\", \"Pflegekräfte\", \"Belegschaft\", \"Angestellte\"],\n",
        "    \"Pflege\": [\"Betreuung\", \"Fürsorge\", \"Versorgung\", \"Hilfe\", \"Obhut\"],\n",
        "    \"freundlich\": [\"nett\", \"höflich\", \"herzlich\", \"zuvorkommend\", \"liebenswürdig\"],\n",
        "    \"unfreundlich\": [\"kalt\", \"respektlos\", \"rüde\", \"abweisend\"],\n",
        "    \"schön\": [\"hübsch\", \"ansprechend\", \"gemütlich\", \"anheimelnd\"],\n",
        "    \"gut\": [\"ordentlich\", \"zufriedenstellend\", \"positiv\", \"lobenswert\"],\n",
        "    \"schlecht\": [\"mies\", \"mangelhaft\", \"unzureichend\", \"fragwürdig\"],\n",
        "    \"Essen\": [\"Speisen\", \"Nahrung\", \"Mahlzeiten\", \"Gerichte\"],\n",
        "    \"lecker\": [\"köstlich\", \"schmackhaft\", \"gut\"],\n",
        "    \"sauber\": [\"gepflegt\", \"rein\", \"ordentlich\", \"hygienisch\"],\n",
        "    \"hilfsbereit\": [\"unterstützend\", \"kooperativ\", \"engagiert\", \"aufmerksam\"],\n",
        "    \"im\": [\"in\", \"bei\"],\n",
        "    \"und\": [\"sowie\", \"–\", \"&\"],\n",
        "    \"sehr\": [\"wirklich\", \"besonders\", \"äußerst\"],\n",
        "    \"ist\": [\"bleibt\", \"erscheint\"],\n",
        "    \"Haus\": [\"Heim\", \"Einrichtung\"],\n",
        "    \"Heim\": [\"Haus\", \"Pflegeheim\", \"Institution\", \"Residenz\"],\n",
        "    \"Mutter\": [\"Familienangehörige\"],\n",
        "    \"Oma\": [\"Großmutter\", \"Omi\"],\n",
        "    \"Opa\": [\"Großvater\"],\n",
        "    \"kompetent\": [\"professionell\", \"fähig\", \"tüchtig\"],\n",
        "    \"nett\": [\"sympathisch\", \"herzlich\", \"angenehm\"],\n",
        "    \"Zimmer\": [\"Räume\", \"Wohnbereich\", \"Appartement\"],\n",
        "    \"Bewohner\": [\"Senioren\", \"Heimbewohner\", \"Pflegebedürftige\"],\n",
        "    \"Einrichtung\": [\"Institution\", \"Haus\", \"Pflegeheim\"],\n",
        "    \"motiviert\": [\"engagiert\", \"eifrig\", \"tatkräftig\"],\n",
        "    \"zugewandt\": [\"aufmerksam\", \"herzlich\", \"fürsorglich\"],\n",
        "    \"Kommunikation\": [\"Austausch\", \"Kontakt\", \"Gespräche\"],\n",
        "    \"Angehörigen\": [\"Familienangehörigen\", \"Verwandten\"],\n",
        "    \"liebevoll\": [\"herzlich\", \"zärtlich\", \"fürsorglich\"],\n",
        "    \"gepflegt\": [\"versorgt\", \"betreut\", \"umhegt\"],\n",
        "    \"muss\": [\"soll\", \"hat zu sein\"],\n",
        "    \"wenn\": [\"falls\", \"sobald\"],\n",
        "    \"irgendetwas\": [\"etwas\", \"manchmal etwas\"],\n",
        "    \"vermeintlich\": [\"angeblich\", \"vermeintlich\", \"scheinbar\"],\n",
        "    \"nicht\": [\"nicht\", \"nicht ganz\"],\n",
        "    \"läuft\": [\"funktioniert\", \"klappt\"],\n",
        "    \"kann\": [\"darf\", \"ist möglich\"],\n",
        "    \"jederzeit\": [\"immer\", \"sofort\", \"jederzeit\"],\n",
        "    \"ansprechen\": [\"erwähnen\", \"anbringen\", \"zur Sprache bringen\"],\n",
        "    \"gemeinsame\": [\"zusammen\", \"gemeinsam gefundene\"],\n",
        "    \"Lösung\": [\"Antwort\", \"Ausweg\"],\n",
        "    \"finden\": [\"erreichen\", \"entdecken\"],\n",
        "}\n",
        "\n",
        "#Umdrehung des Dictionaries: Synonyme werden zu Key\n",
        "CANONICAL = {}\n",
        "for key, syns in SYNONYMS.items():\n",
        "    key_lower = key.lower()\n",
        "    CANONICAL[key_lower] = key_lower\n",
        "    for s in syns:\n",
        "        CANONICAL[s.lower()] = key_lower\n",
        "\n",
        "#Synonyme ersetzen lassen\n",
        "def synonym_replace(text):\n",
        "    result = text\n",
        "    words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "    replaced = set()\n",
        "    for word in words:\n",
        "        if word in CANONICAL and word not in replaced:\n",
        "            main = CANONICAL[word]\n",
        "            candidates = [s for s in SYNONYMS.get(main.capitalize(), []) if s.lower() != word]\n",
        "            if candidates:\n",
        "                repl = random.choice(candidates)\n",
        "                pattern = re.compile(rf'\\b{re.escape(word)}\\b', re.IGNORECASE)\n",
        "                result = pattern.sub(\n",
        "                    lambda m: repl.capitalize() if m.group(0)[0].isupper() else repl,\n",
        "                    result,\n",
        "                    count=1\n",
        "                )\n",
        "                replaced.add(word)\n",
        "    return result\n",
        "\n",
        "#Satz umstrukturieren\n",
        "def restructure_sentences(text):\n",
        "    if len(text) < 100:\n",
        "        return text\n",
        "    sents = sent_tokenize(text)\n",
        "    if len(sents) <= 1:\n",
        "        return text\n",
        "    if random.random() < 0.75:\n",
        "        if len(sents) == 2:\n",
        "            return sents[1] + \" \" + sents[0]\n",
        "        else:\n",
        "            first, last = sents[0], sents[-1]\n",
        "            middle = sents[1:-1]\n",
        "            if middle:\n",
        "                random.shuffle(middle)\n",
        "            return first + \" \" + \" \".join(middle) + \" \" + last\n",
        "    return text\n",
        "\n",
        "#pro Rezension 3 Paraphrasen erstellen\n",
        "def generate_paraphrases(text):\n",
        "    original = text.strip()\n",
        "    if len(original) < 20:\n",
        "        return [original] * 3\n",
        "\n",
        "    results = set()\n",
        "    attempts = 0\n",
        "    max_attempts = 80\n",
        "\n",
        "    while len(results) < 3 and attempts < max_attempts:\n",
        "        step1 = synonym_replace(original)\n",
        "        if len(step1) > 80 and random.random() < 0.65:\n",
        "            para = restructure_sentences(step1)\n",
        "        else:\n",
        "            para = step1\n",
        "\n",
        "        if para != original and len(para) > len(original) * 0.6:\n",
        "            results.add(para)\n",
        "        attempts += 1\n",
        "\n",
        "    result_list = list(results)\n",
        "    while len(result_list) < 3:\n",
        "        result_list.append(original)\n",
        "    return result_list[:3]\n",
        "\n",
        "#Daten aus \"erfahrungen_clean.csv hochladen\"\n",
        "input_path = \"/content/drive/MyDrive/erfahrungen_clean.csv\"\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Sicherstellen, dass Spalte existiert\n",
        "if 'Erfahrungsbericht des Nutzers' not in df.columns or 'Zufallszahl' not in df.columns:\n",
        "  raise ValueError(\"Spalten 'Erfahrungsbericht des Nutzers' oder 'Zufallszahl' fehlen!\")\n",
        "\n",
        "\n",
        "df = df.dropna(subset=['Erfahrungsbericht des Nutzers']).reset_index(drop=True)\n",
        "df['Zufallszahl'] = df['Zufallszahl'].astype(int)\n",
        "\n",
        "print(f\"Geladene Rezensionen: {len(df)}\")\n",
        "# in csv\n",
        "output_path = \"/content/drive/MyDrive/Projekt_ABSA/erfahrungen_paraphrased.csv\"\n",
        "\n",
        "# Liste für alle Zeilen sammeln\n",
        "data_rows = []\n",
        "\n",
        "print(\"Erzeuge Paraphrasen und bereite CSV vor...\")\n",
        "\n",
        "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Paraphrasen erstellen\"):\n",
        "    original_id = int(row['Zufallszahl'])\n",
        "    original_text = str(row['Erfahrungsbericht des Nutzers']).strip()\n",
        "\n",
        "    # 1. Original eintragen\n",
        "    data_rows.append({\n",
        "        'id': original_id,\n",
        "        'text': original_text,\n",
        "        'typ': 'original'\n",
        "    })\n",
        "\n",
        "    # 2. Drei Paraphrasen erzeugen und eintragen\n",
        "    paraphrases = generate_paraphrases(original_text)\n",
        "    for i, para_text in enumerate(paraphrases, 1):\n",
        "        data_rows.append({\n",
        "            'id': original_id,\n",
        "            'text': para_text.strip(),\n",
        "            'typ': f'paraphrase_{i}'\n",
        "        })\n",
        "\n",
        "# In DataFrame umwandeln und als CSV speichern\n",
        "result_df = pd.DataFrame(data_rows)\n",
        "\n",
        "# Reihenfolge der Spalten festlegen\n",
        "result_df = result_df[['id', 'text', 'typ']]\n",
        "\n",
        "# Speichern mit smartEncoding()\n",
        "result_df.to_csv(output_path, index=False, encoding=smartEncoding())\n",
        "\n",
        "print(f\"\\nFERTIG! CSV erfolgreich gespeichert:\")\n",
        "print(f\"   → {output_path}\")\n",
        "print(f\"   → {len(df)} Originale + {len(df)*3} Paraphrasen = {len(result_df)} Zeilen insgesamt\")\n",
        "\n",
        "# Optional: direkter Download im Colab\n",
        "from google.colab import files\n",
        "print(\"\\nStarte Download der Datei...\")\n",
        "files.download(output_path)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9c6QYSff4oz8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MecGjdw--8eqoUfF53VL3Vja6L0ROuUF",
      "authorship_tag": "ABX9TyNrf5twBtFRtdB1GPKZeecW"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}