{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6sxMd7fsqOR"
      },
      "source": [
        "# Gewählte xlsl Datei in csv Datei umwandeln und umbenennen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Schritt 1: Deinstalliere alle problematischen Pakete\n",
        "!pip uninstall tensorflow tensorflow-text tf-keras tensorflow-decision-forests tensorflow-probability -y\n",
        "\n",
        "# Schritt 2: Installiere kompatible Versionen\n",
        "!pip install tensorflow==2.19.0 tensorflow-probability==0.25.0 tensorflow-text==2.19.0 tf-keras==2.19.0\n",
        "\n",
        "# Schritt 3: Unterdrücke Warnungen (für parakeet und CUDA)\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Unterdrückt TensorFlow-Warnungen\n",
        "\n",
        "# Schritt 4: Teste den Import\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "print(\"Import erfolgreich!\")\n",
        "\n",
        "# Schritt 5: Teste tensorflow_probability\n",
        "import tensorflow_probability as tfp\n",
        "print(f\"TensorFlow Probability Version: {tfp.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhCh6T2Crh3u",
        "outputId": "d101cd37-5886-4e3a-ad02-b4e8f028b2ea",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "Found existing installation: tensorflow-text 2.19.0\n",
            "Uninstalling tensorflow-text-2.19.0:\n",
            "  Successfully uninstalled tensorflow-text-2.19.0\n",
            "Found existing installation: tf_keras 2.19.0\n",
            "Uninstalling tf_keras-2.19.0:\n",
            "  Successfully uninstalled tf_keras-2.19.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-decision-forests as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow-probability 0.25.0\n",
            "Uninstalling tensorflow-probability-0.25.0:\n",
            "  Successfully uninstalled tensorflow-probability-0.25.0\n",
            "Collecting tensorflow==2.19.0\n",
            "  Using cached tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-probability==0.25.0\n",
            "  Using cached tensorflow_probability-0.25.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tensorflow-text==2.19.0\n",
            "  Using cached tensorflow_text-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting tf-keras==2.19.0\n",
            "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability==0.25.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability==0.25.0) (3.1.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability==0.25.0) (0.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->tensorflow-probability==0.25.0) (25.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
            "Using cached tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
            "Using cached tensorflow_probability-0.25.0-py2.py3-none-any.whl (7.0 MB)\n",
            "Using cached tensorflow_text-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
            "Installing collected packages: tensorflow-probability, tensorflow, tf-keras, tensorflow-text\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [tensorflow-text]\n",
            "\u001b[1A\u001b[2KSuccessfully installed tensorflow-2.19.0 tensorflow-probability-0.25.0 tensorflow-text-2.19.0 tf-keras-2.19.0\n",
            "Import erfolgreich!\n",
            "TensorFlow Probability Version: 0.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow_probability as tfp\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from google.colab import drive\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n"
      ],
      "metadata": {
        "id": "VhqmpRX0F-WE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smartEncoding():\n",
        "  plt = platform.system\n",
        "  if plt == \"Windows\":\n",
        "    return \"utf-8-sig\"\n",
        "  else:\n",
        "    return \"utf-8\""
      ],
      "metadata": {
        "id": "NqgyTXTb2AqA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VaFrz040hSDp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "read_file = pd.DataFrame(pd.read_excel('/content/drive/My Drive/Google_Rezensionen_qualitätsgesichert_2023_12_19 (1).xlsx'))\n",
        "read_file.to_csv(\"Google_Rezensionen_qualitätsgesichert_2023_12_19 (1).csv\", index = False, header = True, encoding=smartEncoding())\n",
        "df = pd.DataFrame(pd.read_csv(\"Google_Rezensionen_qualitätsgesichert_2023_12_19 (1).csv\"))\n",
        "#print(df.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "q4FshCWEs1-J"
      },
      "outputs": [],
      "source": [
        "#wähle relevante Spalten\n",
        "gewaehlte_spalten = df[[\"Erfahrungsbericht des Nutzers\" , \"Zufallszahl\"]]\n",
        "\n",
        "#speichere sie in neue csv datei\n",
        "gewaehlte_spalten.to_csv(\"/content/drive/My Drive/erfahrungen_gefiltert.csv\", index = False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfd5RwKdjNQv"
      },
      "source": [
        "# Datenbereinigung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "etZLmS_XhRox",
        "outputId": "202314f6-980c-4370-da37-4a14abae9c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL: 6453 Reihen\n",
            "100 Zufallsberichte!\n"
          ]
        }
      ],
      "source": [
        "erfahrungen_gefiltert = pd.read_csv(\"/content/drive/My Drive/erfahrungen_gefiltert.csv\")\n",
        "# dropna() entfernt NaN-Reihen aus anderen Spalten\n",
        "erfahrungen_gefiltert.dropna(subset=['Erfahrungsbericht des Nutzers'], inplace=True)\n",
        "# leere strings entfernen\n",
        "erfahrungen_gefiltert.drop_duplicates(inplace=True)\n",
        "#speichern\n",
        "erfahrungen_gefiltert.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index=False)\n",
        "print(f\"FINAL: {len(erfahrungen_gefiltert)} Reihen\")\n",
        "\n",
        "\n",
        "#speichere sie in neue csv datei\n",
        "gewaehlte_spalten.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index = False)\n",
        "\n",
        "\n",
        "#zufällig n = 100 Berichte für erfahrungen_final.csv auswählen\n",
        "erfahrungen_clean = pd.read_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\")\n",
        "# dropna() entfernt NaN-Reihen aus anderen Spalten\n",
        "erfahrungen_clean.dropna(subset=['Erfahrungsbericht des Nutzers'], inplace=True)\n",
        "# leere strings entfernen\n",
        "erfahrungen_clean.drop_duplicates(inplace=True)\n",
        "erfahrungen_clean.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index = False)\n",
        "\n",
        "# INDEX-Nummern zufällig wählen\n",
        "zufalls_index = random.sample(range(len(erfahrungen_clean)), k=100)\n",
        "zufall_100 = erfahrungen_clean.iloc[zufalls_index]\n",
        "\n",
        "\n",
        "\n",
        "#speichern (NUR 100!)\n",
        "zufall_100.to_csv(\"/content/drive/My Drive/zufall_100_berichte.csv\", index=False)\n",
        "\n",
        "print(f\"{len(zufall_100)} Zufallsberichte!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPzrsAJjFsW2"
      },
      "source": [
        "# **Datenaugmentation und Syn-Chain-Methode**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datenaugmentation mit Trainingsdaten (ca. 11.900 Rezensionen)"
      ],
      "metadata": {
        "id": "oDX0N5V-IhnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "für Fine Tuning:\n",
        "Hugging Face: Fine-tune ein Grok-ähnliches Modell (z. B. \"xai-org/grok-1\" oder \"reedmayhew/Grok-3-gemma3-4B-distilled\" als Distillation von Grok 3). Das ist open-source und GitHub-freundlich.\n",
        "\n",
        "Code-Beispiel (Distillation von Grok 3 via API + Fine-Tuning auf Gemma-3 4B):\n"
      ],
      "metadata": {
        "id": "vfZm6aOvPbPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers torch datasets accelerate huggingface_hub\n",
        "\n",
        "#from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "#from datasets import Dataset\n",
        "#import torch\n",
        "\n",
        "# Lade und bereite Daten (aus vorherigem Code)\n",
        "#data = pd.read_csv(\"/content/drive/My Drive/Rezensionen/erfahrungen_clean.csv\")\n",
        "#data['Label'] = data['Erfahrungsbericht des Nutzers'].apply(get_sentiment)\n",
        "#data['Label'] = data['Label'].map({'positiv': 0, 'neutral': 1, 'negativ': 2})  # Numerisch\n",
        "\n",
        "# Augmentierte Trainingsdaten (aus vorherigem Code, ca. 11.900 + augmentiert)\n",
        "# train_data_augmented, test_data = ... (aus vorherigem Split)\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained('google/gemma-3-4b')  # Grok-ähnlich\n",
        "#def tokenize(batch):\n",
        "    #return tokenizer(batch['Erfahrungsbericht des Nutzers'], padding=True, truncation=True)\n",
        "\n",
        "#train_dataset = Dataset.from_pandas(train_data_augmented).map(tokenize, batched=True)\n",
        "#test_dataset = Dataset.from_pandas(test_data).map(tokenize, batched=True)\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained('google/gemma-3-4b', num_labels=3)\n",
        "\n",
        "#training_args = TrainingArguments(\n",
        "    #output_dir='/content/drive/My Drive/Rezensionen/grok3_finetuned',\n",
        "    #num_train_epochs=3,\n",
        "    #per_device_train_batch_size=4,  # Passe an GPU an\n",
        "    #evaluation_strategy='epoch',\n",
        "    #save_strategy='epoch',\n",
        "    #load_best_model_at_end=True,\n",
        "    #push_to_hub=True  # Direkt zu HF Hub pushen (GitHub-ähnlich)\n",
        "#)\n",
        "\n",
        "#trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset, tokenizer=tokenizer)\n",
        "#trainer.train()\n",
        "\n",
        "# Push zu Hugging Face Hub (braucht HF-Token)\n",
        "#trainer.push_to_hub(\"dein-username/grok3-mini-finetuned-sentiment\", token=\"DEIN_HF_TOKEN\")\n",
        "#print(\"Modell auf Hugging Face Hub hochgeladen – kann zu GitHub-Repo verlinkt werden.\")"
      ],
      "metadata": {
        "id": "2GBzX7elPmyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Practices (aus Reddit, GitHub Blog, HF Docs):\n",
        "\n",
        "Lizenz: Füge eine LICENSE (z. B. Apache 2.0) hinzu, da Grok-Modelle proprietär sind – dein Fine-Tuning erbt das.\n",
        "Model Card: Erstelle eine (HF-Template: https://huggingface.co/docs/hub/model-cards) mit Details zu Daten (12.000 Rezensionen), Augmentation und Metriken.\n",
        "Datenschutz: Anonymisiere sensible Daten in der CSV (z. B. Namen entfernen), bevor du hochlädst – Pflegeberichte könnten personenbezogen sein (DSGVO-konform!).\n",
        "Versionskontrolle: Nutze GitHub Releases für große Dateien (z. B. Weights als Asset).\n",
        "Integration mit HF: Push zu Hugging Face Hub und verlinke im GitHub-Repo – HF ist GitHub-ähnlich und unterstützt Grok-Destillationen (z. B. \"reedmayhew/Grok-3-gemma3-4B-distilled\").\n",
        "\n",
        "\n",
        "\n",
        "3. Empfehlung für deine Bachelorarbeit\n",
        "\n",
        "Starte mit Option A/B: Prompt Engineering für schnelle Tests, dann Hugging Face für echtes Fine-Tuning (ca. 30–60 Min. in Colab mit GPU).\n",
        "GitHub-Repo als Portfolio: Lade Code, Skripte und Metriken hoch – das zeigt Reproduzierbarkeit. Vermeide rohe Weights (>100 MB) direkt; verlinke sie.\n",
        "Nächste Schritte: Hole dir einen xAI API-Key und HF-Token. Teste den Code in Colab.\n",
        "\n",
        "Falls du Hilfe beim Setup (z. B. API-Key, spezifischer Code-Fehler) brauchst oder mehr Details zu einer Option, lass es mich wissen!"
      ],
      "metadata": {
        "id": "V0ywlg4TQWl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paraphrasieren"
      ],
      "metadata": {
        "id": "8p0Z4ygud_8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#csv Datei laden\n",
        "data = pd.read_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\")\n",
        "\n",
        "# Erstellung Pegasus-Modell für Paraphrasing (kompatibel mit Colab)\n",
        "model_name = \"google/pegasus-x-base\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def paraphrase_text(text):\n",
        "  try:\n",
        "    inputs = tokenizer(f\"paraphrase: {text}\", max_length=128, truncation = True, return_tensors = \"pt\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length = 128,\n",
        "        num_beams = 5,\n",
        "        early_stopping = True\n",
        "    )\n",
        "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
        "    return paraphrased_text\n",
        "  except Exception as e:\n",
        "    print(f\"Fehler beim Paraphrase für folgenden Text: {text}[:50]...- {e}\")\n",
        "    return text\n",
        "#Paraphrasieren von Trainingsdaten (Demo: 10, für 12.000 für GPU Colab Pro)\n",
        "paraphrased_texts = []\n",
        "for idx, row in data.head(10).iterrows():#ersetze head(an Stelle 10) mit data\n",
        "  text = row['Erfahrungsbericht des Nutzers']\n",
        "  zufallszahl = row['Zufallszahl']\n",
        "  paraphrased_text = paraphrase_text(text)\n",
        "  paraphrased_texts.append({\n",
        "        'Original': text,\n",
        "        'Paraphrase': paraphrased_text,\n",
        "        'Zufallszahl': zufallszahl\n",
        "    })\n",
        "\n",
        "# In DataFrame speichern\n",
        "paraphrased_df = pd.DataFrame(paraphrased_texts)\n",
        "\n",
        "# Speichere in Google Drive\n",
        "ordner = \"/content/drive/My Drive/Rezensionen/\"\n",
        "os.makedirs(ordner, exist_ok=True)\n",
        "alter_pfad = ordner + \"paraphrased_pegasus_temp.csv\"\n",
        "paraphrased_df.to_csv(alter_pfad, index=False)\n",
        "neuer_pfad = ordner + f\"paraphrased_pegasus_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "os.rename(alter_pfad, neuer_pfad)\n",
        "print(f\"Paraphrasierte Daten gespeichert als '{neuer_pfad}'.\")\n",
        "\n",
        "\n",
        "#Paraphrasing und Grammatical Variation\n",
        "#Synonymersetzung (nlpaug) & generative Paraphrasierung (german-t5-efficient).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VFs7qGcmIsWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a838894-5523-4dd1-e764-93e14ba51537"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type pegasus_x to instantiate a model of type pegasus. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-x-base and are newly initialized: ['model.decoder.embed_positions.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.10.encoder_attn.k_proj.bias', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.11.encoder_attn.k_proj.bias', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.8.encoder_attn.k_proj.bias', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.9.encoder_attn.k_proj.bias', 'model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paraphrasierte Daten gespeichert als '/content/drive/My Drive/Rezensionen/paraphrased_pegasus_20251022_224002.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b06VD1uYFlSq"
      },
      "outputs": [],
      "source": [
        "# um damit zu beginnen: https://arxiv.org/html/2507.09485"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1MecGjdw--8eqoUfF53VL3Vja6L0ROuUF",
      "authorship_tag": "ABX9TyMaQqt6p4K3x5WkSYSIVvwL"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}