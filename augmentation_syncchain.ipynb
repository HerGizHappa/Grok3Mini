{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9BCNOvT1auc"
      },
      "source": [
        "# Setup & Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nhCh6T2Crh3u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "b75658a2-a2fd-4556-d388-3772b303ae52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Schritt 1: Deinstalliere alle problematischen Pakete\\n!pip uninstall tensorflow tensorflow-text tf-keras tensorflow-decision-forests tensorflow-probability -y\\n\\n# Schritt 2: Installiere kompatible Versionen\\n!pip install tensorflow==2.19.0 tensorflow-probability==0.25.0 tensorflow-text==2.19.0 tf-keras==2.19.0\\n\\n!pip install -q sentence-transformers transformers accelerate torch pandas tqdm\\n\\n# Schritt 3: Unterdrücke Warnungen (für parakeet und CUDA)\\nimport os\\nos.environ[\\'TF_CPP_MIN_LOG_LEVEL\\'] = \\'2\\'  # Unterdrückt TensorFlow-Warnungen\\n\\n# Schritt 4: Teste den Import\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\\nprint(\"Import erfolgreich!\")\\n\\n# Schritt 5: Teste tensorflow_probability\\nimport tensorflow_probability as tfp\\nprint(f\"TensorFlow Probability Version: {tfp.__version__}\")\\n!pip install transformers torch pandas tqdm -q'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "'''# Schritt 1: Deinstalliere alle problematischen Pakete\n",
        "!pip uninstall tensorflow tensorflow-text tf-keras tensorflow-decision-forests tensorflow-probability -y\n",
        "\n",
        "# Schritt 2: Installiere kompatible Versionen\n",
        "!pip install tensorflow==2.19.0 tensorflow-probability==0.25.0 tensorflow-text==2.19.0 tf-keras==2.19.0\n",
        "\n",
        "!pip install -q sentence-transformers transformers accelerate torch pandas tqdm\n",
        "\n",
        "# Schritt 3: Unterdrücke Warnungen (für parakeet und CUDA)\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Unterdrückt TensorFlow-Warnungen\n",
        "\n",
        "# Schritt 4: Teste den Import\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "print(\"Import erfolgreich!\")\n",
        "\n",
        "# Schritt 5: Teste tensorflow_probability\n",
        "import tensorflow_probability as tfp\n",
        "print(f\"TensorFlow Probability Version: {tfp.__version__}\")\n",
        "!pip install transformers torch pandas tqdm -q'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VhqmpRX0F-WE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import platform #infos aus dem System verarbeiten können (Windows)\n",
        "import os #für Dateihändelung\n",
        "import pandas as pd #für Datenanalyse (Bereinigung von Daten)\n",
        "import random\n",
        "from tqdm import tqdm #um ausgeben zu können, wie weit die Prozesse laufen (für Prozess Balken)\n",
        "import tensorflow_probability as tfp # Wahrscheinlichekti von Ergebnissen analysieren & bewerten\n",
        "import torch # soll dabei helfen, schnellere Berechnungen zu machen (GPU)\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM #aus hugging face bibliothek, um Texte in Tokens umzuwandeln, Textsequenzen als Input und andere Textsequenzen ausgeben (Output)\n",
        "#Aufgabe: Übersetzen, Zusammenfassen, Generieren\n",
        "from sentence_transformers import SentenceTransformer, util #sentenceTranformer, damit werden Modelle geladen, die Texte in ganze Vektoren umwandeln\n",
        "#ür Sätze/Texte erzeugen → Ähnlichkeitsvergleiche\n",
        "from google.colab import files, drive #Interaktion mit lokalen Dateien in Google Colab Umgebung\n",
        "from datetime import datetime# Manipulation von Zeitangaben\n",
        "import shutil #Operationen mit Dateien und mit dem Sammeln von Dateien\n",
        "import re #Operationen für reguläre Ausdrücke (regex)\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "# NLTK Punkt-Tokenizer\n",
        "nltk.download('punkt', quiet=True)\n",
        "import math\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "API_KEY = os.getenv(\"GROK_API_KEY\")\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"Set GROK_API_KEY in .env\")\n",
        "\n",
        "print(\"API key loaded securely\")"
      ],
      "metadata": {
        "id": "PMo7y8cZSTX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d75fae-fa64-4bb1-8c69-80d815cec399"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded securely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo \".env\" >> .gitignore"
      ],
      "metadata": {
        "id": "zkXpz8_VRhOF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NqgyTXTb2AqA"
      },
      "outputs": [],
      "source": [
        "def smartEncoding():\n",
        "  plt = platform.system\n",
        "  if plt == \"Windows\":\n",
        "    return \"utf-8-sig\"\n",
        "  else:\n",
        "    return \"utf-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6sxMd7fsqOR"
      },
      "source": [
        "# Gewählte xlsl Datei in csv Datei umwandeln und umbenennen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VaFrz040hSDp"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "read_file = pd.DataFrame(pd.read_excel('/content/drive/My Drive/Google_Rezensionen_qualitätsgesichert_2023_12_19 (1).xlsx'))\n",
        "read_file.to_csv(\"Google_Rezensionen_qualitätsgesichert_2023_12_19 (1).csv\", index = False, header = True, encoding=smartEncoding())\n",
        "df = pd.DataFrame(pd.read_csv(\"Google_Rezensionen_qualitätsgesichert_2023_12_19 (1).csv\"))\n",
        "#print(df.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q4FshCWEs1-J"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#wähle relevante Spalten\n",
        "gewaehlte_spalten = df[[\"Erfahrungsbericht des Nutzers\" , \"Zufallszahl\"]]\n",
        "\n",
        "#speichere sie in neue csv datei\n",
        "gewaehlte_spalten.to_csv(\"/content/drive/My Drive/erfahrungen_gefiltert.csv\", index = False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfd5RwKdjNQv"
      },
      "source": [
        "# Datenbereinigung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "etZLmS_XhRox",
        "outputId": "0cf77e72-16ff-46b8-bef8-672fe51bc91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL: 6453 Reihen\n",
            "100 Zufallsberichte!\n"
          ]
        }
      ],
      "source": [
        "erfahrungen_gefiltert = pd.read_csv(\"/content/drive/My Drive/erfahrungen_gefiltert.csv\")\n",
        "# dropna() entfernt NaN-Reihen aus anderen Spalten\n",
        "erfahrungen_gefiltert.dropna(subset=['Erfahrungsbericht des Nutzers'], inplace=True)\n",
        "# leere strings entfernen\n",
        "erfahrungen_gefiltert.drop_duplicates(inplace=True)\n",
        "#speichern\n",
        "erfahrungen_gefiltert.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index=False)# Entfernung von Duplikaten wurde beim df selbst vollzogen anstatt als Kopie\n",
        "print(f\"FINAL: {len(erfahrungen_gefiltert)} Reihen\")\n",
        "\n",
        "\n",
        "#speichere sie in neue csv datei\n",
        "gewaehlte_spalten.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index = False)\n",
        "\n",
        "\n",
        "#zufällig n = 100 Berichte für erfahrungen_final.csv auswählen\n",
        "erfahrungen_clean = pd.read_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\")\n",
        "# dropna() entfernt NaN-Reihen aus anderen Spalten\n",
        "erfahrungen_clean.dropna(subset=['Erfahrungsbericht des Nutzers'], inplace=True)\n",
        "# leere strings entfernen\n",
        "erfahrungen_clean.drop_duplicates(inplace=True)\n",
        "erfahrungen_clean.to_csv(\"/content/drive/My Drive/erfahrungen_clean.csv\", index = False)\n",
        "\n",
        "# INDEX-Nummern zufällig wählen\n",
        "zufalls_index = random.sample(range(len(erfahrungen_clean)), k=100)\n",
        "zufall_100 = erfahrungen_clean.iloc[zufalls_index]\n",
        "\n",
        "\n",
        "\n",
        "#speichern (NUR 100!)\n",
        "zufall_100.to_csv(\"/content/drive/My Drive/zufall_100_berichte.csv\", index=False)\n",
        "\n",
        "print(f\"{len(zufall_100)} Zufallsberichte!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPzrsAJjFsW2"
      },
      "source": [
        "# **Datenaugmentation und Syn-Chain-Methode**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDX0N5V-IhnY"
      },
      "source": [
        "## Datenaugmentation mit Trainingsdaten (ca. 11.900 Rezensionen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p0Z4ygud_8_"
      },
      "source": [
        "## Paraphrasieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkXOEJrCaMv2",
        "outputId": "34c2bd9f-4d30-4ba8-cea8-465db3b01ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Ordner & Bibliotheken geladen!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Google Drive mounten/ Ordner in Google Colab Umbegung erstellen lassen (einmalige Ausführung nötig)\n",
        "drive.mount('/content/drive')\n",
        "SAVE_PATH = '/content/drive/MyDrive/Projekt_ABSA' #Unterordner \"Projekt_ABSA\" in Google Colab Umgebung erstellen\n",
        "os.makedirs(SAVE_PATH, exist_ok=True) #Methode, wobei ein Projektverzeichnis (Pfad für Paraphrase mit inbegriffen) festgelegt wird\n",
        "\n",
        "print(\"Ordner & Bibliotheken geladen!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sieht nice aus"
      ],
      "metadata": {
        "id": "fDHUXUvU3mLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smartEncoding():\n",
        "  plt = platform.system\n",
        "  if plt == \"Windows\":\n",
        "    return \"utf-8-sig\"\n",
        "  else:\n",
        "    return \"utf-8\"\n",
        "\n",
        "# Erstellung eines \"Synonym-Dictionarys\", zuerste mit Schlüsselwert wie z.B. \"schmutzig\" & danach Wertpaaren\n",
        "SYNONYMS = {\n",
        "    \"Personal\": [\"Team\", \"Mitarbeiter\", \"Pflegekräfte\", \"Belegschaft\", \"Angestellte\"],\n",
        "    \"Pflege\": [\"Betreuung\", \"Fürsorge\", \"Versorgung\", \"Hilfe\", \"Obhut\"],\n",
        "    \"freundlich\": [\"nett\", \"höflich\", \"herzlich\", \"zuvorkommend\", \"liebenswürdig\"],\n",
        "    \"unfreundlich\": [\"kalt\", \"respektlos\", \"rüde\", \"abweisend\"],\n",
        "    \"schön\": [\"hübsch\", \"ansprechend\", \"gemütlich\", \"anheimelnd\"],\n",
        "    \"gut\": [\"ordentlich\", \"zufriedenstellend\", \"positiv\", \"lobenswert\"],\n",
        "    \"schlecht\": [\"mies\", \"mangelhaft\", \"unzureichend\", \"fragwürdig\"],\n",
        "    \"Essen\": [\"Speisen\", \"Nahrung\", \"Mahlzeiten\", \"Gerichte\"],\n",
        "    \"lecker\": [\"köstlich\", \"schmackhaft\", \"gut\"],\n",
        "    \"sauber\": [\"gepflegt\", \"rein\", \"ordentlich\", \"hygienisch\"],\n",
        "    \"hilfsbereit\": [\"unterstützend\", \"kooperativ\", \"engagiert\", \"aufmerksam\"],\n",
        "    \"im\": [\"in\", \"bei\"],\n",
        "    \"und\": [\"sowie\", \"–\", \"&\"],\n",
        "    \"sehr\": [\"wirklich\", \"besonders\", \"äußerst\"],\n",
        "    \"ist\": [\"bleibt\", \"erscheint\"],\n",
        "    \"Haus\": [\"Heim\", \"Einrichtung\"],\n",
        "    \"Heim\": [\"Haus\", \"Pflegeheim\", \"Institution\", \"Residenz\"],\n",
        "    \"Mutter\": [\"Familienangehörige\"],\n",
        "    \"Oma\": [\"Großmutter\", \"Omi\"],\n",
        "    \"Opa\": [\"Großvater\"],\n",
        "    \"kompetent\": [\"professionell\", \"fähig\", \"tüchtig\"],\n",
        "    \"nett\": [\"sympathisch\", \"herzlich\", \"angenehm\"],\n",
        "    \"Zimmer\": [\"Räume\", \"Wohnbereich\", \"Appartement\"],\n",
        "    \"Bewohner\": [\"Senioren\", \"Heimbewohner\", \"Pflegebedürftige\"],\n",
        "    \"Einrichtung\": [\"Institution\", \"Haus\", \"Pflegeheim\"],\n",
        "    \"motiviert\": [\"engagiert\", \"eifrig\", \"tatkräftig\"],\n",
        "    \"zugewandt\": [\"aufmerksam\", \"herzlich\", \"fürsorglich\"],\n",
        "    \"Kommunikation\": [\"Austausch\", \"Kontakt\", \"Gespräche\"],\n",
        "    \"Angehörigen\": [\"Familienangehörigen\", \"Verwandten\"],\n",
        "    \"liebevoll\": [\"herzlich\", \"zärtlich\", \"fürsorglich\"],\n",
        "    \"gepflegt\": [\"versorgt\", \"betreut\", \"umhegt\"],\n",
        "    \"muss\": [\"soll\", \"hat zu sein\"],\n",
        "    \"wenn\": [\"falls\", \"sobald\"],\n",
        "    \"irgendetwas\": [\"etwas\", \"manchmal etwas\"],\n",
        "    \"vermeintlich\": [\"angeblich\", \"vermeintlich\", \"scheinbar\"],\n",
        "    \"nicht\": [\"nicht\", \"nicht ganz\"],\n",
        "    \"läuft\": [\"funktioniert\", \"klappt\"],\n",
        "    \"kann\": [\"darf\", \"ist möglich\"],\n",
        "    \"jederzeit\": [\"immer\", \"sofort\", \"jederzeit\"],\n",
        "    \"ansprechen\": [\"erwähnen\", \"anbringen\", \"zur Sprache bringen\"],\n",
        "    \"gemeinsame\": [\"zusammen\", \"gemeinsam gefundene\"],\n",
        "    \"Lösung\": [\"Antwort\", \"Ausweg\"],\n",
        "    \"finden\": [\"erreichen\", \"entdecken\"],\n",
        "}\n",
        "\n",
        "CANONICAL = {}\n",
        "for key, syns in SYNONYMS.items():\n",
        "    key_lower = key.lower()\n",
        "    CANONICAL[key_lower] = key_lower\n",
        "    for s in syns:\n",
        "        CANONICAL[s.lower()] = key_lower\n",
        "\n",
        "#Synonyme ersetzen lassen\n",
        "def synonym_replace(text):\n",
        "    result = text\n",
        "    words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "    replaced = set()\n",
        "    for word in words:\n",
        "        if word in CANONICAL and word not in replaced:\n",
        "            main = CANONICAL[word]\n",
        "            candidates = [s for s in SYNONYMS.get(main.capitalize(), []) if s.lower() != word]\n",
        "            if candidates:\n",
        "                repl = random.choice(candidates)\n",
        "                pattern = re.compile(rf'\\b{re.escape(word)}\\b', re.IGNORECASE)\n",
        "                result = pattern.sub(\n",
        "                    lambda m: repl.capitalize() if m.group(0)[0].isupper() else repl,\n",
        "                    result,\n",
        "                    count=1\n",
        "                )\n",
        "                replaced.add(word)\n",
        "    return result\n",
        "\n",
        "#Satz umstrukturieren\n",
        "def restructure_sentences(text):\n",
        "    if len(text) < 100:\n",
        "        return text\n",
        "    sents = sent_tokenize(text)\n",
        "    if len(sents) <= 1:\n",
        "        return text\n",
        "    if random.random() < 0.75:\n",
        "        if len(sents) == 2:\n",
        "            return sents[1] + \" \" + sents[0]\n",
        "        else:\n",
        "            first, last = sents[0], sents[-1]\n",
        "            middle = sents[1:-1]\n",
        "            if middle:\n",
        "                random.shuffle(middle)\n",
        "            return first + \" \" + \" \".join(middle) + \" \" + last\n",
        "    return text\n",
        "\n",
        "#pro Rezension 3 Paraphrasen erstellen\n",
        "def generate_paraphrases(text):\n",
        "    original = text.strip()\n",
        "    if len(original) < 20:\n",
        "        return [original] * 3\n",
        "\n",
        "    results = set()\n",
        "    attempts = 0\n",
        "    max_attempts = 80\n",
        "\n",
        "    while len(results) < 3 and attempts < max_attempts:\n",
        "        step1 = synonym_replace(original)\n",
        "        if len(step1) > 80 and random.random() < 0.65:\n",
        "            para = restructure_sentences(step1)\n",
        "        else:\n",
        "            para = step1\n",
        "\n",
        "        if para != original and len(para) > len(original) * 0.6:\n",
        "            results.add(para)\n",
        "        attempts += 1\n",
        "\n",
        "    result_list = list(results)\n",
        "    while len(result_list) < 3:\n",
        "        result_list.append(original)\n",
        "    return result_list[:3]\n",
        "\n",
        "#Daten aus \"erfahrungen_clean.csv hochladen\"\n",
        "input_path = \"/content/drive/MyDrive/erfahrungen_clean.csv\"\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Sicherstellen, dass Spalte existiert\n",
        "if 'Erfahrungsbericht des Nutzers' not in df.columns:\n",
        "    raise ValueError(\"Spalte 'Erfahrungsbericht des Nutzers' nicht gefunden!\")\n",
        "\n",
        "df = df.dropna(subset=['Erfahrungsbericht des Nutzers']).reset_index(drop=True)\n",
        "df = df.reset_index().rename(columns={'index': 'id'})\n",
        "\n",
        "print(f\"Geladene Rezensionen: {len(df)}\")\n",
        "\n",
        "#in JSONL Datei speichern\n",
        "output_path = \"/content/drive/MyDrive/Projekt_ABSA/erfahrungen_paraphrased.jsonl\"\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for _, row in df.iterrows():\n",
        "        original_id = int(row['id'])\n",
        "        original_text = str(row['Erfahrungsbericht des Nutzers'])\n",
        "\n",
        "        # 1. Originalzeile\n",
        "        original_entry = {\n",
        "            \"id\": original_id,\n",
        "            \"original\": original_text\n",
        "        }\n",
        "        f.write(json.dumps(original_entry, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        # 2. 3 Paraphrasen pro Rezension\n",
        "        paraphrases = generate_paraphrases(original_text)\n",
        "        for i, para in enumerate(paraphrases, 1):\n",
        "            para_entry = {\n",
        "                \"variant_id\": i,\n",
        "                \"paraphrase\": para\n",
        "            }\n",
        "            f.write(json.dumps(para_entry, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"\\nFERTIG! JSONL gespeichert:\")\n",
        "print(f\"   → {output_path}\")\n",
        "print(f\"   → {len(df)} Originale + {len(df)*3} Paraphrasen = {len(df)*4} Zeilen\")\n",
        "\n",
        "#download der paraphrasierten Rezensionen\n",
        "from google.colab import files\n",
        "print(\"\\nStarte Download...\")\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "collapsed": true,
        "id": "9c6QYSff4oz8",
        "outputId": "26434cc2-de06-44dd-f4bd-07e72361199a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Geladene Rezensionen: 6453\n",
            "\n",
            "FERTIG! JSONL gespeichert:\n",
            "   → /content/drive/MyDrive/Projekt_ABSA/erfahrungen_paraphrased.jsonl\n",
            "   → 6453 Originale + 19359 Paraphrasen = 25812 Zeilen\n",
            "\n",
            "Starte Download...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ca817710-b508-4fe9-ad06-eb2e68f93ff2\", \"erfahrungen_paraphrased.jsonl\", 7833418)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nan-Werte entfernen aus JSONL-Datei"
      ],
      "metadata": {
        "id": "In9SANSCO_lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_file = \"/content/sample_data/data_training.jsonl\"\n",
        "output_file = \"cleaned_jsonl.jsonl\"\n",
        "\n",
        "def find_nan(obj):\n",
        "    if isinstance(obj, float):\n",
        "        return math.isnan(obj) or math.isinf(obj)\n",
        "    if isinstance(obj, dict):\n",
        "        return any(find_nan(v) for v in obj.values())\n",
        "    if isinstance(obj, list):\n",
        "        return any(find_nan(v) for v in obj)\n",
        "    return False\n",
        "\n",
        "with open(input_file, 'r', encoding=smartEncoding()) as f, \\\n",
        "     open(output_file, 'w', encoding=smartEncoding()) as fi:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "            if not find_nan(obj):\n",
        "                fi.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n",
        "        except json.JSONDecodeError:\n",
        "            pass  # skip invalid lines\n"
      ],
      "metadata": {
        "id": "3LICNoAXWoNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Trainingsdaten und Testdaten aufteilen**"
      ],
      "metadata": {
        "id": "46bLyqJFqTsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Anwendung der Syn-Chain-Methode** - beim Fine Tuning"
      ],
      "metadata": {
        "id": "sb3_zwk7ZkOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/drive/MyDrive/cleaned_jsonl_training.jsonl, daran weiter arbeiten\n",
        "\n"
      ],
      "metadata": {
        "id": "oq36fhL5Zqdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfZm6aOvPbPD"
      },
      "source": [
        "für Fine Tuning:\n",
        "Hugging Face: Fine-tune ein Grok-ähnliches Modell (z. B. \"xai-org/grok-1\" oder \"reedmayhew/Grok-3-gemma3-4B-distilled\" als Distillation von Grok 3). Das ist open-source und GitHub-freundlich.\n",
        "\n",
        "Code-Beispiel (Distillation von Grok 3 via API + Fine-Tuning auf Gemma-3 4B):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ywlg4TQWl2"
      },
      "source": [
        "Best Practices (aus Reddit, GitHub Blog, HF Docs):\n",
        "\n",
        "Lizenz: Füge eine LICENSE (z. B. Apache 2.0) hinzu, da Grok-Modelle proprietär sind – dein Fine-Tuning erbt das.\n",
        "Model Card: Erstelle eine (HF-Template: https://huggingface.co/docs/hub/model-cards) mit Details zu Daten (12.000 Rezensionen), Augmentation und Metriken.\n",
        "Datenschutz: Anonymisiere sensible Daten in der CSV (z. B. Namen entfernen), bevor du hochlädst – Pflegeberichte könnten personenbezogen sein (DSGVO-konform!).\n",
        "Versionskontrolle: Nutze GitHub Releases für große Dateien (z. B. Weights als Asset).\n",
        "Integration mit HF: Push zu Hugging Face Hub und verlinke im GitHub-Repo – HF ist GitHub-ähnlich und unterstützt Grok-Destillationen (z. B. \"reedmayhew/Grok-3-gemma3-4B-distilled\").\n",
        "\n",
        "\n",
        "\n",
        "3. Empfehlung für deine Bachelorarbeit\n",
        "\n",
        "Starte mit Option A/B: Prompt Engineering für schnelle Tests, dann Hugging Face für echtes Fine-Tuning (ca. 30–60 Min. in Colab mit GPU).\n",
        "GitHub-Repo als Portfolio: Lade Code, Skripte und Metriken hoch – das zeigt Reproduzierbarkeit. Vermeide rohe Weights (>100 MB) direkt; verlinke sie.\n",
        "Nächste Schritte: Hole dir einen xAI API-Key und HF-Token. Teste den Code in Colab.\n",
        "\n",
        "Falls du Hilfe beim Setup (z. B. API-Key, spezifischer Code-Fehler) brauchst oder mehr Details zu einer Option, lass es mich wissen!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b06VD1uYFlSq"
      },
      "outputs": [],
      "source": [
        "# um damit zu beginnen: https://arxiv.org/html/2507.09485"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MecGjdw--8eqoUfF53VL3Vja6L0ROuUF",
      "authorship_tag": "ABX9TyPO107UDWqjI2MQDN0m0bIs"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}